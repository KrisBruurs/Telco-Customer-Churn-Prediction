---
title: "Telco Customer Churn Prediction"
author: "Kris Bruurs"
format: html
editor: visual
---

# 1. About the Document

------------------------------------------------------------------------

# 2. Libraries

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(skimr)
library(janitor)
library(tidymodels)
```

------------------------------------------------------------------------

# 3. Load and Audit the Dataset

------------------------------------------------------------------------

## 3.1 Load Data

Load Data in R and clean names for easy data handling.

```{r}
churn_data <- read_csv('data/WA_Fn-UseC_-Telco-Customer-Churn.csv') %>% 
  clean_names()
```

------------------------------------------------------------------------

## 3.2 Structure Preview

```{r}
glimpse(churn_data)
```

```{r}
skim(churn_data)
```

## 3.3 Explore Data

------------------------------------------------------------------------

Make sure `churn` variable is a factor.

```{r}
churn_data <- churn_data %>% 
  mutate(churn = as.factor(churn))

churn_data <- churn_data %>%
  mutate(churn = fct_relevel(churn, "Yes"))
```

------------------------------------------------------------------------

Class balance for churn variable.

```{r}
churn_data %>% 
  count(churn) %>% 
  mutate(prop = round(n / sum(n), 2))
```

Data shows that 27% of customers churn and 73% stay.

------------------------------------------------------------------------

Missing values scan.

```{r}
missing_by_col <- churn_data %>% 
  summarise(across(everything(), ~sum(is.na(.)))) %>% 
  pivot_longer(everything(), names_to = 'column', values_to = 'na_count') %>% 
  arrange(desc(na_count))

missing_by_col %>% 
  head(10)

```

The only column with missing values is `total_charges`.

------------------------------------------------------------------------

Check for duplicates in the data.

```{r}
nrow(churn_data)
```

```{r}
n_distinct(churn_data$customer_id)
```

Both show an equal number, meaning the data does not consist of duplicates.

------------------------------------------------------------------------

# 4. Split and Prepare Data

------------------------------------------------------------------------

## 4.1 Split Data

------------------------------------------------------------------------

```{r}
set.seed(123) # Ensure reproducibility

churn_split <- initial_split(churn_data, prop = 0.8, strata = churn)
churn_train <- training(churn_split)
churn_test <- testing(churn_split)
```

------------------------------------------------------------------------

Check if proportions in training and testing data are similar to complete dataset

```{r}
count(churn_train, churn) %>% 
  mutate(prop = round(n/sum(n),2))
```

```{r}
count(churn_test, churn) %>% 
  mutate(prop = round(n/sum(n),2))
```

Both of the above tables shows that the proportions in the training and testing data are equal to the full data.

------------------------------------------------------------------------

## 4.2 Preprocess

------------------------------------------------------------------------

Create a churn recipe where:

-   All missing values in `total_charges` are changed to the median

-   All nominal variables become dummy variables

-   All variables with zero variance are removed

```{r}
churn_recipe <- recipe(churn ~ ., data = churn_train) %>% 
  update_role(customer_id, new_role = 'ID') %>% 
  step_impute_median(total_charges) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors())
```

# 5. Baseline Logistic Regression

------------------------------------------------------------------------

## 5.1 Define Model

------------------------------------------------------------------------

```{r}
model1 <- logistic_reg(mode = 'classification') %>% 
  set_engine('glm')
```

------------------------------------------------------------------------

## 5.2 Combine Model with Recipe into Workflow

------------------------------------------------------------------------

```{r}
reg_wf <- workflow() %>% 
  add_recipe(churn_recipe) %>% 
  add_model(model1)
```

## 5.3 Fit the Model

```{r}
reg_fit <- fit(reg_wf, data = churn_data)
```

## 5.4 Evaluate on Test Set

```{r}
reg_preds <- predict(reg_fit, churn_test, type = "prob") %>%
  bind_cols(predict(reg_fit, churn_test)) %>%
  bind_cols(churn_test %>% select(churn))
```

Confusion matrix:

```{r}
reg_preds %>%
  conf_mat(truth = churn, estimate = .pred_class)
```

```{r}
reg_preds %>%
  roc_auc(truth = churn, .pred_Yes)
```

# 6. Evaluate Model Performance

```{r}
tp = 220
tn = 935
fp = 100
fn = 154
```

## 6.1 Accuracy

$$
(TP + TN) / (TP+FP+FN+TN)
$$

```{r}
accuracy <- round((tp+tn)/(tp+fp+fn+tn) * 100, 2)

print(paste('Accuracy:', accuracy))
```

The overall model predicts about 82% of cases correctly.

## 6.2 Precision

$$
TP/(TP+FP)
$$

```{r}
precision <- round(tp/(tp+fp)*100, 2)

print(paste('Precision:', precision))
```

When the model predicts if a customer will churn, its right in about 69% of cases.

## 6.3 Sensitivity

$$
TP/(TP+FN)
$$

```{r}
sensitivity <- round(tp/(tp+fn) * 100, 2)

print(paste('Sensitivity:', sensitivity))
```

The model catches about 59% of actual churners.

## 6.4 F1 Score

$$
2* {\\frac{Precision*Sensitivity}{Precision+Sensitivity}}
$$
